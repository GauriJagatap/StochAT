{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Adversarial Training (StochAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanila SGD: \n",
    "MNIST - 99%+ (most cnns), CIFAR10 - 93%+ (resnet18), 96%+ (wideresnet) \n",
    "\n",
    "MNIST:\n",
    "\n",
    "adversarial attacks: \n",
    "l-inf @ eps = 80/255 @20 steps: TRADES - 96.07% - (4 layer cnn), MART 96.4%, MMA 95.5%, PGD - 96.01% - (4 layer cnn)\n",
    "\n",
    "adversarial attacks:\n",
    "l-2 @ eps = 32/255 (check): TRADES, MMA, PGD\n",
    "\n",
    "CIFAR10:\n",
    "\n",
    "adversarial attacks: \n",
    "l-inf @ eps = 8/255 @20 steps: \n",
    "TRADES 53-56% - (WRN-34-10), MART 57-58% (WRN-34-10), MMA 47%, PGD 48% - (WRN-32-10)// 49% - (WRN-34-10), Std - 0.03%\n",
    "https://openreview.net/pdf?id=rklOg6EFwS (Table 4)\n",
    "\n",
    "adversarial attacks: \n",
    "l-inf @ eps = 8/255 @20 steps: \n",
    "[ResNet10] TRADES 45.4%, MART 46.6%, MMA 37.26%, PGD 42.27%, Std 0.14%\n",
    "\n",
    "Benign accuracies: TRADES 84.92%, MART 83.62%, MMA 84.36, PGD 87.14%, Std 95.8% [wideresnet]\n",
    "https://openreview.net/pdf?id=Ms9zjhVB5R (Table 1)\n",
    "\n",
    "adversarial attacks:\n",
    "l-2 @ eps = 32/255 (check): TRADES, MART, MMA, PGD\n",
    "\n",
    "TBD: CWinf attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained models for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download pretrained models and place in ../trainedmodels/MNIST or ../trainedmodels/CIFAR10 respectively\n",
    "\n",
    "### TRADES :\n",
    "https://github.com/yaodongyu/TRADES (MNIST: small cnn, CIFAR10: WideResNet34)\n",
    "### MMA : \n",
    "https://github.com/BorealisAI/mma_training (MNIST: lenet5, CIFAR10: WideResNet28)\n",
    "### MART :\n",
    " https://github.com/YisenWang/MART (CIFAR10: ResNet18 and WideResNet34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from multiprocessing import cpu_count\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import olympic\n",
    "from typing import Union, Callable, Tuple\n",
    "import sys\n",
    "sys.path.append('../adversarial/')\n",
    "sys.path.append('../architectures/')\n",
    "from functional import boundary, iterated_fgsm, local_search, pgd, entropySmoothing\n",
    "from ESGD_utils import *\n",
    "import pickle\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import argparse, math, random\n",
    "import ESGD_optim\n",
    "from trades import trades_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place data folders outside working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = 'CIFAR10' # [MNIST, CIFAR10]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "bsz = 128\n",
    "if dataset == 'MNIST':\n",
    "    train = datasets.MNIST('../../data/MNIST', train=True, transform=transform, download=True)\n",
    "    val = datasets.MNIST('../../data/MNIST', train=False, transform=transform, download=True)\n",
    "elif dataset == 'CIFAR10':\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train = datasets.CIFAR10('../../data/CIFAR10', train=True, transform=transform_train, download=True)\n",
    "    val = datasets.CIFAR10('../../data/CIFAR10', train=False, transform=transform_test, download=True)\n",
    "     \n",
    "train_loader = DataLoader(train, batch_size=128, shuffle=True, **kwargs)\n",
    "val_loader = DataLoader(val, batch_size=128, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALIZE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='MNIST':\n",
    "    from small_cnn import SmallCNN   \n",
    "    Net = SmallCNN\n",
    "    NetName = 'SmallCNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='CIFAR10':\n",
    "    #[ResNet18,ResNet34,ResNet50,WideResNet]\n",
    "    from resnet import ResNet18,ResNet34,ResNet50\n",
    "    from wideresnet import WideResNet\n",
    "    Net = WideResNet\n",
    "    NetName = 'WideResNet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM SEED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['test_batch_size'] = 128\n",
    "args['no_cuda'] = False\n",
    "args['epsilon'] = 0.031\n",
    "args['num_steps'] = 10\n",
    "args['step_size'] = 0.007\n",
    "args['random'] =True,\n",
    "args['white_box_attack']=True\n",
    "args['log_interval'] = 100\n",
    "args['beta'] = 6.0\n",
    "args['seed'] = 1\n",
    "args['lr'] = 0.1\n",
    "args['momentum'] = 0.9\n",
    "args['epochs'] = 76\n",
    "args['batch_size'] = 128\n",
    "args['save_freq'] = 3\n",
    "args['weight_decay'] = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9d46f4b5e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = args['seed']\n",
    "torch.set_num_threads(2)\n",
    "if DEVICE=='cuda':\n",
    "    torch.cuda.set_device(-1)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    cudnn.benchmark = True\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_train(model, device, train_loader):\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            train_loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('Training: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "    training_accuracy = correct / len(train_loader.dataset)\n",
    "    return train_loss, training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pgd_whitebox(model,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=0.031,\n",
    "                  num_steps=20,\n",
    "                  step_size=0.003):\n",
    "    out = model(X)\n",
    "    err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "    if args['random']:\n",
    "        random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(DEVICE)\n",
    "        X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            loss = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "        loss.backward()\n",
    "        eta = step_size * X_pgd.grad.data.sign()\n",
    "        X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "        eta = torch.clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "        X_pgd = Variable(X.data + eta, requires_grad=True)\n",
    "        X_pgd = Variable(torch.clamp(X_pgd, 0, 1.0), requires_grad=True)\n",
    "    err_pgd = (model(X_pgd).data.max(1)[1] != y.data).float().sum()\n",
    "    #print('err pgd (white-box): ', err_pgd)\n",
    "    return err, err_pgd\n",
    "\n",
    "def eval_adv_test_whitebox(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    evaluate model by white-box attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    robust_err_total = 0\n",
    "    natural_err_total = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # pgd attack\n",
    "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
    "        err_natural, err_robust = _pgd_whitebox(model, X, y)\n",
    "        robust_err_total += err_robust\n",
    "        natural_err_total += err_natural\n",
    "    print('natural_acc_total: ', 100-natural_err_total.item()/100)\n",
    "    print('robust_acc_total: ', 100-robust_err_total.item()/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../trainedmodels'):\n",
    "    os.makedirs('../trainedmodels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infnorm(x):\n",
    "    infn = torch.max(torch.abs(x.detach().cpu()))\n",
    "    return infn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial(method,model, device, train_loader, optimizer, epoch,adversary,k,step,eps,norm,random):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        ypred = model(data)\n",
    "        \n",
    "        sgd_loss = nn.CrossEntropyLoss()\n",
    "        # calculate robust loss\n",
    "        loss = method(model,optimizer,sgd_loss,data,target,epoch,adversary,k,step,eps,norm,random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL USING TRADES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-nn-epoch10.pt  model-nn-epoch61.pt\r\n",
      "model-nn-epoch15.pt  model-nn-epoch64.pt\r\n",
      "model-nn-epoch20.pt  \u001b[0m\u001b[01;31mopt-nn-checkpoint_epoch26.tar\u001b[0m\r\n",
      "model-nn-epoch25.pt  \u001b[01;31mopt-nn-checkpoint_epoch30.tar\u001b[0m\r\n",
      "model-nn-epoch26.pt  \u001b[01;31mopt-nn-checkpoint_epoch35.tar\u001b[0m\r\n",
      "model-nn-epoch30.pt  \u001b[01;31mopt-nn-checkpoint_epoch40.tar\u001b[0m\r\n",
      "model-nn-epoch35.pt  \u001b[01;31mopt-nn-checkpoint_epoch45.tar\u001b[0m\r\n",
      "model-nn-epoch40.pt  \u001b[01;31mopt-nn-checkpoint_epoch50.tar\u001b[0m\r\n",
      "model-nn-epoch45.pt  \u001b[01;31mopt-nn-checkpoint_epoch52.tar\u001b[0m\r\n",
      "model-nn-epoch50.pt  \u001b[01;31mopt-nn-checkpoint_epoch55.tar\u001b[0m\r\n",
      "model-nn-epoch52.pt  \u001b[01;31mopt-nn-checkpoint_epoch58.tar\u001b[0m\r\n",
      "model-nn-epoch55.pt  \u001b[01;31mopt-nn-checkpoint_epoch61.tar\u001b[0m\r\n",
      "model-nn-epoch58.pt  \u001b[01;31mopt-nn-checkpoint_epoch64.tar\u001b[0m\r\n",
      "model-nn-epoch5.pt\r\n"
     ]
    }
   ],
   "source": [
    "ls ../WRN_TRADES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_TRADESInf = Net().to(DEVICE)\n",
    "model_TRADESInf.load_state_dict(torch.load('../WRN_TRADES/model-nn-epoch64.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_train(model_TRADESInf, DEVICE, train_loader)\n",
    "#eval_adv_test_whitebox(model_TRADESInf, DEVICE, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trades(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate robust loss\n",
    "        loss = trades_loss(model=model,\n",
    "                           x_natural=data,\n",
    "                           y=target,\n",
    "                           optimizer=optimizer,\n",
    "                           step_size=args['step_size'],\n",
    "                           epsilon=args['epsilon'],\n",
    "                           perturb_steps=args['num_steps'],\n",
    "                           beta=args['beta'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # print progress\n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"decrease the learning rate\"\"\"\n",
    "    lr = args['lr']\n",
    "    if epoch >= 55:\n",
    "        lr = args['lr'] * 0.1\n",
    "    if epoch >= 64:\n",
    "        lr = args['lr'] * 0.01\n",
    "    if epoch >= 100:\n",
    "        lr = args['lr'] * 0.001\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../WRN_TRADES'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65\n",
      "lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 65 [0/50000 (0%)]\tLoss: 0.718938\n",
      "Train Epoch: 65 [12800/50000 (26%)]\tLoss: 0.720713\n",
      "Train Epoch: 65 [25600/50000 (51%)]\tLoss: 0.718890\n",
      "Train Epoch: 65 [38400/50000 (77%)]\tLoss: 0.868327\n",
      "================================================================\n",
      "Training: Average loss: 0.4007, Accuracy: 48081/50000 (96%)\n",
      "natural_acc_total:  83.71000000000001\n",
      "robust_acc_total:  53.79\n",
      "================================================================\n",
      "Epoch: 66\n",
      "lr: 0.001\n",
      "Train Epoch: 66 [0/50000 (0%)]\tLoss: 0.885656\n",
      "Train Epoch: 66 [12800/50000 (26%)]\tLoss: 0.886515\n",
      "Train Epoch: 66 [25600/50000 (51%)]\tLoss: 0.738653\n",
      "Train Epoch: 66 [38400/50000 (77%)]\tLoss: 0.758270\n",
      "================================================================\n",
      "Training: Average loss: 0.4088, Accuracy: 47896/50000 (96%)\n",
      "natural_acc_total:  83.75\n",
      "robust_acc_total:  53.95\n",
      "================================================================\n",
      "Epoch: 67\n",
      "lr: 0.001\n",
      "Train Epoch: 67 [0/50000 (0%)]\tLoss: 0.728145\n",
      "Train Epoch: 67 [12800/50000 (26%)]\tLoss: 0.722494\n",
      "Train Epoch: 67 [25600/50000 (51%)]\tLoss: 0.807068\n",
      "Train Epoch: 67 [38400/50000 (77%)]\tLoss: 0.786618\n",
      "================================================================\n",
      "Training: Average loss: 0.3834, Accuracy: 48135/50000 (96%)\n",
      "natural_acc_total:  84.09\n",
      "robust_acc_total:  53.36\n",
      "================================================================\n",
      "Epoch: 68\n",
      "lr: 0.001\n",
      "Train Epoch: 68 [0/50000 (0%)]\tLoss: 0.720449\n",
      "Train Epoch: 68 [12800/50000 (26%)]\tLoss: 0.782598\n",
      "Train Epoch: 68 [25600/50000 (51%)]\tLoss: 0.663270\n",
      "Train Epoch: 68 [38400/50000 (77%)]\tLoss: 0.730079\n",
      "================================================================\n",
      "Training: Average loss: 0.3818, Accuracy: 48193/50000 (96%)\n",
      "natural_acc_total:  83.73\n",
      "robust_acc_total:  53.78\n",
      "================================================================\n",
      "Epoch: 69\n",
      "lr: 0.001\n",
      "Train Epoch: 69 [0/50000 (0%)]\tLoss: 0.727082\n",
      "Train Epoch: 69 [12800/50000 (26%)]\tLoss: 0.754244\n",
      "Train Epoch: 69 [25600/50000 (51%)]\tLoss: 0.708816\n",
      "Train Epoch: 69 [38400/50000 (77%)]\tLoss: 0.596165\n",
      "================================================================\n",
      "Training: Average loss: 0.3769, Accuracy: 48151/50000 (96%)\n",
      "natural_acc_total:  83.94\n",
      "robust_acc_total:  54.1\n",
      "================================================================\n",
      "Epoch: 70\n",
      "lr: 0.001\n",
      "Train Epoch: 70 [0/50000 (0%)]\tLoss: 0.711128\n",
      "Train Epoch: 70 [12800/50000 (26%)]\tLoss: 0.671423\n",
      "Train Epoch: 70 [25600/50000 (51%)]\tLoss: 0.703999\n",
      "Train Epoch: 70 [38400/50000 (77%)]\tLoss: 0.760931\n",
      "================================================================\n",
      "Training: Average loss: 0.3725, Accuracy: 48227/50000 (96%)\n",
      "natural_acc_total:  83.88\n",
      "robust_acc_total:  53.7\n",
      "================================================================\n",
      "Epoch: 71\n",
      "lr: 0.001\n",
      "Train Epoch: 71 [0/50000 (0%)]\tLoss: 0.664049\n",
      "Train Epoch: 71 [12800/50000 (26%)]\tLoss: 0.703570\n",
      "Train Epoch: 71 [25600/50000 (51%)]\tLoss: 0.681979\n",
      "Train Epoch: 71 [38400/50000 (77%)]\tLoss: 0.790572\n",
      "================================================================\n",
      "Training: Average loss: 0.3677, Accuracy: 48269/50000 (97%)\n",
      "natural_acc_total:  83.77\n",
      "robust_acc_total:  53.83\n",
      "================================================================\n",
      "Epoch: 72\n",
      "lr: 0.001\n",
      "Train Epoch: 72 [0/50000 (0%)]\tLoss: 0.616657\n",
      "Train Epoch: 72 [12800/50000 (26%)]\tLoss: 0.673824\n",
      "Train Epoch: 72 [25600/50000 (51%)]\tLoss: 0.691496\n",
      "Train Epoch: 72 [38400/50000 (77%)]\tLoss: 0.688230\n",
      "================================================================\n",
      "Training: Average loss: 0.3617, Accuracy: 48283/50000 (97%)\n",
      "natural_acc_total:  83.82\n",
      "robust_acc_total:  53.83\n",
      "================================================================\n",
      "Epoch: 73\n",
      "lr: 0.001\n",
      "Train Epoch: 73 [0/50000 (0%)]\tLoss: 0.674485\n",
      "Train Epoch: 73 [12800/50000 (26%)]\tLoss: 0.699683\n",
      "Train Epoch: 73 [25600/50000 (51%)]\tLoss: 0.699526\n",
      "Train Epoch: 73 [38400/50000 (77%)]\tLoss: 0.799597\n",
      "================================================================\n",
      "Training: Average loss: 0.3587, Accuracy: 48323/50000 (97%)\n",
      "natural_acc_total:  83.91\n",
      "robust_acc_total:  53.24\n",
      "================================================================\n",
      "Epoch: 74\n",
      "lr: 0.001\n",
      "Train Epoch: 74 [0/50000 (0%)]\tLoss: 0.644193\n",
      "Train Epoch: 74 [12800/50000 (26%)]\tLoss: 0.664399\n",
      "Train Epoch: 74 [25600/50000 (51%)]\tLoss: 0.520120\n",
      "Train Epoch: 74 [38400/50000 (77%)]\tLoss: 0.638453\n",
      "================================================================\n",
      "Training: Average loss: 0.3618, Accuracy: 48365/50000 (97%)\n",
      "natural_acc_total:  83.86\n",
      "robust_acc_total:  53.59\n",
      "================================================================\n",
      "Epoch: 75\n",
      "lr: 0.001\n",
      "Train Epoch: 75 [0/50000 (0%)]\tLoss: 0.691638\n",
      "Train Epoch: 75 [12800/50000 (26%)]\tLoss: 0.662697\n",
      "Train Epoch: 75 [25600/50000 (51%)]\tLoss: 0.659883\n",
      "Train Epoch: 75 [38400/50000 (77%)]\tLoss: 0.678799\n",
      "================================================================\n",
      "Training: Average loss: 0.3595, Accuracy: 48320/50000 (97%)\n",
      "natural_acc_total:  84.08\n",
      "robust_acc_total:  53.57\n",
      "================================================================\n",
      "Epoch: 76\n",
      "lr: 0.001\n",
      "Train Epoch: 76 [0/50000 (0%)]\tLoss: 0.848350\n",
      "Train Epoch: 76 [12800/50000 (26%)]\tLoss: 0.640694\n",
      "Train Epoch: 76 [25600/50000 (51%)]\tLoss: 0.678522\n",
      "Train Epoch: 76 [38400/50000 (77%)]\tLoss: 0.731227\n",
      "================================================================\n",
      "Training: Average loss: 0.3508, Accuracy: 48364/50000 (97%)\n",
      "natural_acc_total:  83.84\n",
      "robust_acc_total:  53.18\n",
      "================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-84bab1cd48ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                        os.path.join(model_dir, 'opt-nn-checkpoint_epoch{}.tar'.format(epoch)))\n\u001b[1;32m     30\u001b[0m \u001b[0;31m## save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodelname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../trainedmodels/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mNetName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_TRADESInf_ep'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_lr'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_TRADESInf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodelname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": [
    "#### args['attack'] = 'l_inf' \n",
    "## initialize model\n",
    "#model_TRADESInf = Net().to(DEVICE)\n",
    "## training params\n",
    "optimizer = optim.SGD(model_TRADESInf.parameters(), lr=args['lr'], momentum=args['momentum'], weight_decay=args['weight_decay'])\n",
    "epochs = 76\n",
    "## train model\n",
    "\n",
    "for epoch in range(65, epochs + 1):\n",
    "    print('Epoch:',epoch)\n",
    "    # adjust learning rate for SGD\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print('lr:',param_group['lr'])\n",
    "    # adversarial training\n",
    "    train_trades(args, model_TRADESInf, DEVICE, train_loader, optimizer, epoch)\n",
    "\n",
    "    # evaluation on natural examples\n",
    "    if (epoch-1) % 1 == 0:\n",
    "        print('================================================================')\n",
    "        eval_train(model_TRADESInf, DEVICE, train_loader)\n",
    "        eval_adv_test_whitebox(model_TRADESInf, DEVICE, val_loader)\n",
    "        print('================================================================')\n",
    "\n",
    "    if (epoch-1) % args['save_freq'] == 0:\n",
    "        torch.save(model_TRADESInf.state_dict(),\n",
    "               os.path.join(model_dir, 'model-nn-epoch{}.pt'.format(epoch)))\n",
    "        torch.save(optimizer.state_dict(),\n",
    "                       os.path.join(model_dir, 'opt-nn-checkpoint_epoch{}.tar'.format(epoch)))\n",
    "## save model\n",
    "modelname = '../trainedmodels/'+dataset+'/'+NetName+'_TRADESInf_ep'+str(epochs)+'_lr'+str(lr)+'.pt'\n",
    "torch.save(model_TRADESInf,modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train(model_TRADESInf, DEVICE, train_loader)\n",
    "eval_test(model_TRADESInf, DEVICE, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_TRADESInf = Net().to(DEVICE)\n",
    "model_TRADESInf.load_state_dict(torch.load('../WRN_TRADES/model-nn-epoch5.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
