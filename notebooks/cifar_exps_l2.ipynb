{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main script for training a classifier for CIFAR-10 using l_2 ATENT [Table 4 and Table 5 of paper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Training via ENTropic regularization (l_2 ATENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanila SGD: \n",
    "MNIST - 99%+ (most cnns), CIFAR10 - 93%+ (resnet18), 96%+ (wideresnet) \n",
    "\n",
    "MNIST:\n",
    "\n",
    "adversarial attacks:\n",
    "l-2 @ eps = 2 PGD @40 steps: MMA - 73.02\n",
    "\n",
    "CIFAR10:\n",
    "\n",
    "adversarial attacks:\n",
    "l-2 @ eps = 128/255: MMA 67%, PGD 68%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference repos for baselines: TRADES : https://github.com/yaodongyu/TRADES (MNIST: small cnn, CIFAR10: WideResNet34) MMA : https://github.com/BorealisAI/mma_training (MNIST: lenet5, CIFAR10: WideResNet28) MART : https://github.com/YisenWang/MART (CIFAR10: ResNet18 and WideResNet34) PGD: (CIFAR10: ResNet50) https://github.com/MadryLab/robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append('../adversarial/')\n",
    "sys.path.append('../architectures/')\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functional import entropySmoothing\n",
    "from utils import eval_train, eval_test, infnorm, train_adversarial, project\n",
    "from cifar_resnet import resnet as resnet_cifar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET TRAINING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "#data loading\n",
    "args['seed'] = 2\n",
    "args['test_batch_size'] = 256\n",
    "args['train_batch_size'] = 256\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "args['no_cuda'] = False\n",
    "\n",
    "if not args['no_cuda']:\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = 'cuda'\n",
    "    else:\n",
    "        DEVICE = 'cpu'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# params for SGLD (inner loop)\n",
    "args['attack'] = 'l_2'\n",
    "args['norm'] = 2\n",
    "args['epsilon'] = 0.435\n",
    "args['num_steps'] = 10\n",
    "args['step_size'] = 0.5 * args['epsilon']/args['num_steps']\n",
    "args['random'] = True\n",
    "\n",
    "# params for SGD (outer loop)\n",
    "args['lr'] = 0.1\n",
    "args['momentum'] = 0.9\n",
    "args['weight_decay'] = 1e-4\n",
    "args['epochs'] = 54\n",
    "args['save_freq'] = 1\n",
    "\n",
    "# load model\n",
    "args['pretrained'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = 'CIFAR10' # [MNIST, CIFAR10]\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train = datasets.CIFAR10('../../data/CIFAR10', train=True, transform=transform_train, download=True)\n",
    "val = datasets.CIFAR10('../../data/CIFAR10', train=False, transform=transform_test, download=True)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=args['train_batch_size'], shuffle=True, **kwargs)\n",
    "val_loader = DataLoader(val, batch_size=args['test_batch_size'], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='CIFAR10':\n",
    "    #[ResNet18,ResNet34,ResNet50,WideResNet]\n",
    "    from resnet import ResNet18,ResNet34,ResNet50\n",
    "    from wideresnet import WideResNet\n",
    "    Net = ResNet18\n",
    "    #Net = resnet_cifar(depth=20, num_classes=10)\n",
    "    NetName = 'ResNet18'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM SEED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb53015cf10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = args['seed']\n",
    "torch.set_num_threads(2)\n",
    "if DEVICE=='cuda':\n",
    "    torch.cuda.set_device(-1)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    cudnn.benchmark = True\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHITEBOX L-2 ATTACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pgd_whitebox(model,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=args['epsilon'],\n",
    "                  norm=args['norm'],\n",
    "                  num_steps=args['num_steps'],\n",
    "                  step_size=args['step_size']):\n",
    "    out = model(X)\n",
    "    err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "    if args['random']:\n",
    "        random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(DEVICE)\n",
    "        X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            loss = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "        loss.backward()\n",
    "        if norm=='inf':\n",
    "            eta = step_size * X_pgd.grad.data.sign()\n",
    "            X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "            eta = torch.clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "            X_pgd = Variable(X.data + eta, requires_grad=True)\n",
    "        elif norm==2:\n",
    "            eta = step_size * X_pgd.grad.data / X_pgd.grad.view(X_pgd.shape[0], -1).norm(2, dim=-1)\\\n",
    "                    .view(-1, 1, 1, 1)\n",
    "            X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "            X_pgd = project(X, X_pgd, norm, epsilon)            \n",
    "        X_pgd = Variable(torch.clamp(X_pgd, 0, 1.0), requires_grad=True)\n",
    "    err_pgd = (model(X_pgd).data.max(1)[1] != y.data).float().sum()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_pgd = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "    #print('err pgd (white-box): ', err_pgd)\n",
    "    return err, err_pgd, loss_pgd.item()\n",
    "\n",
    "def eval_adv_test_whitebox(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    evaluate model by white-box attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    robust_err_total = 0\n",
    "    natural_err_total = 0\n",
    "    lossrob  = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # pgd attack\n",
    "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
    "        err_natural, err_robust, losspgd = _pgd_whitebox(model, X, y)\n",
    "        robust_err_total += err_robust\n",
    "        natural_err_total += err_natural\n",
    "        lossrob = lossrob + losspgd\n",
    "    rob = 100-100*robust_err_total.item()/len(test_loader.dataset)   \n",
    "    lossrob /= len(test_loader)\n",
    "    print('robust test loss:',lossrob)\n",
    "    print('natural_acc_total: ', 100-100*natural_err_total.item()/len(test_loader.dataset))\n",
    "    print('robust_acc_total: ', rob)\n",
    "    return rob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-2 ATENT MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training_entropy(model, optimiser, loss_fn, x, y, epoch, adversary, L, step, eps, norm):\n",
    "    \"\"\"Performs a single update against a specified adversary\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Adversial perturbation\n",
    "    alpha=0.9\n",
    "    loss = 0\n",
    "    gamma = 0.05\n",
    "    projector = False #(always false for l2 setup)\n",
    "    \n",
    "    for l in range(L):     \n",
    "        \n",
    "        if l==0: ## initialize using random perturbation of true x, run for one epoch\n",
    "            k=1\n",
    "            random=True\n",
    "            xp = None\n",
    "        elif l>0 and l<L-1: ## initialize with previous iterate of adversarial perturbation, run one epoch\n",
    "            k=1\n",
    "            random=False\n",
    "            xp=x_adv\n",
    "        elif l == L-1: ## initialize with previous iterate, run one epoch, project to epsilon ball\n",
    "            k=1\n",
    "            random=False\n",
    "            xp = x_adv\n",
    "        #eps=0.577 produces effective noise std 0.12    \n",
    "        x_adv,bfl = adversary(model, x, y, loss_fn, xp=xp, step=step, eps=eps, norm=norm, random=random, ep=0.577,projector=projector,gamma=gamma, debug=True)\n",
    "        optimiser.zero_grad()\n",
    "        y_pred = model(x_adv)\n",
    "        pred = y_pred.max(1, keepdim=True)[1]\n",
    "        correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "        \n",
    "        loss_lg = loss_fn(y_pred, y)\n",
    "        loss = (1-alpha)*loss + alpha*loss_lg\n",
    "        \n",
    "        if bfl: #break and readjust learning rate if gradients vanish\n",
    "            break\n",
    "            \n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZE NET OR LOAD FROM CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robust test loss: 1.0569263845682144\n",
      "natural_acc_total:  72.1\n",
      "robust_acc_total:  64.55\n"
     ]
    }
   ],
   "source": [
    "model_ATENT = Net().to(DEVICE)\n",
    "#model_ATENT = nn.DataParallel(model_ATENT)\n",
    "#load pretrained state dict here\n",
    "if args['pretrained']:\n",
    "    pathstr = '../trainedmodels/CIFAR10/smooth-model-l2.pt' \n",
    "    model_ATENT.load_state_dict(torch.load(pathstr))\n",
    "    rob = eval_adv_test_whitebox(model_ATENT, DEVICE, val_loader)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADJUST LEARNING RATE SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch,lr_init):\n",
    "    \"\"\"decrease the learning rate\"\"\"\n",
    "    lr = lr_init\n",
    "    if epoch >= 50:   \n",
    "        lr = lr_init * 0.1\n",
    "    print('lr:',lr)    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATHS TO STORE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../trainedmodels'): #path to save final trained model\n",
    "    os.makedirs('../trainedmodels')\n",
    "model_dir = '../trainedmodels/'+NetName #path to save each epoch while training\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN MODULE USING ATENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not args['pretrained']:    \n",
    "\n",
    "    ## training params\n",
    "    epochs = args['epochs']  \n",
    "    lr_init = args['lr']\n",
    "    #optimizer = optim.Adam(model_ATENT.parameters(), lr=lr_init)\n",
    "\n",
    "    optimizer = optim.SGD(model_ATENT.parameters(), lr=lr_init, momentum=0.9, weight_decay=1e-4)\n",
    "    ## train model\n",
    "\n",
    "    for epoch in range(20, epochs+30):\n",
    "        print('Epoch:',epoch)\n",
    "\n",
    "        # adjust learning rate for SGD\n",
    "        adjust_learning_rate(optimizer, epoch,lr_init)\n",
    "\n",
    "        # adversarial training\n",
    "        train_adversarial(adversarial_training_entropy,model_ATENT, DEVICE, train_loader, \n",
    "                          optimizer, epoch,adversary=entropySmoothing,L=args['num_steps'],\n",
    "                          step=args['step_size'],eps=args['epsilon'],norm=args['norm'])\n",
    "\n",
    "        # evaluation on natural and adversarial examples\n",
    "        print('================================================================')\n",
    "        eval_train(model_ATENT, DEVICE, train_loader)\n",
    "        rob = eval_adv_test_whitebox(model_ATENT, DEVICE, val_loader)            \n",
    "        print('================================================================')\n",
    "\n",
    "        # save checkpoint\n",
    "        torch.save(model_ATENT.state_dict(),\n",
    "                       os.path.join(model_dir, 'model-nn-epoch{}-robacc{}.pt'.format(epoch,int(np.round(rob)))))\n",
    "\n",
    "    ## save model\n",
    "\n",
    "    modelname = '../trainedmodels/'+dataset+'/'+NetName+'_ATENT_'+args['attack']+'_ep'+str(epochs)+'_lr'+str(lr_init)+'.pt'\n",
    "    torch.save(model_ATENT,modelname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
