{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main script for training a classifier for MNIST using l_inf ATENT [Table 2 of paper].\n",
    "\n",
    "Notebook contains printed result from evaluation of pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Training via ENTropic regularization (l_inf ATENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoTA - collected from various papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanila SGD: \n",
    "MNIST - 99%+ (most cnns), CIFAR10 - 93%+ (resnet18), 96%+ (wideresnet) \n",
    "\n",
    "MNIST:\n",
    "\n",
    "adversarial attacks: \n",
    "l-inf @ eps = 80/255 @20 steps: TRADES - 96.07% - (4 layer cnn), MART 96.4%, MMA 95.5%, PGD - 96.01% - (4 layer cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference repos for baselines: TRADES : https://github.com/yaodongyu/TRADES (MNIST: small cnn, CIFAR10: WideResNet34) MMA : https://github.com/BorealisAI/mma_training (MNIST: lenet5, CIFAR10: WideResNet28) MART : https://github.com/YisenWang/MART (CIFAR10: ResNet18 and WideResNet34) PGD: (CIFAR10: ResNet50) https://github.com/MadryLab/robustness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append('../adversarial/')\n",
    "sys.path.append('../architectures/')\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functional import entropySmoothing\n",
    "from utils import eval_train, eval_test, infnorm, train_adversarial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET TRAINING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "#data loading\n",
    "args['seed'] = 1\n",
    "args['test_batch_size'] = 128\n",
    "args['train_batch_size'] = 128\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "args['no_cuda'] = False\n",
    "\n",
    "if not args['no_cuda']:\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = 'cuda'\n",
    "    else:\n",
    "        DEVICE = 'cpu'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# params for SGLD (inner loop)\n",
    "args['attack'] = 'l_inf'\n",
    "args['norm'] = 'inf'\n",
    "args['epsilon'] = 0.3\n",
    "args['num_steps'] = 40\n",
    "args['step_size'] = 0.01\n",
    "args['random'] =True\n",
    "\n",
    "# params for SGD (outer loop)\n",
    "args['lr'] = 0.01\n",
    "args['momentum'] = 0.9\n",
    "args['weight_decay'] = 1e-4\n",
    "args['epochs'] = 30\n",
    "args['save_freq'] = 1\n",
    "\n",
    "# load model\n",
    "args['pretrained'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'MNIST' # [MNIST, CIFAR10]\n",
    "transform = transforms.Compose([\n",
    "transforms.ToTensor()])\n",
    "train = datasets.MNIST('../../data/', train=True, transform=transform, download=True)\n",
    "val = datasets.MNIST('../../data/', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(train, batch_size=args['train_batch_size'], shuffle=True, **kwargs)\n",
    "val_loader = DataLoader(val, batch_size=args['test_batch_size'], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='MNIST':\n",
    "    from small_cnn import SmallCNN   \n",
    "    Net = SmallCNN\n",
    "    NetName = 'SmallCNN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET RANDOM SEED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe04c046f10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = args['seed']\n",
    "torch.set_num_threads(2)\n",
    "if DEVICE=='cuda':\n",
    "    torch.cuda.set_device(-1)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    cudnn.benchmark = True\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHITEBOX L-INF ATTACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pgd_whitebox(model,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=args['epsilon'],\n",
    "                  norm=args['norm'],\n",
    "                  num_steps=args['num_steps'],\n",
    "                  step_size=args['step_size']):\n",
    "    out = model(X)\n",
    "    err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "    if args['random']:\n",
    "        random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(DEVICE)\n",
    "        X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            loss = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "        loss.backward()\n",
    "        if norm=='inf':\n",
    "            eta = step_size * X_pgd.grad.data.sign()\n",
    "            X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "            eta = torch.clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "            X_pgd = Variable(X.data + eta, requires_grad=True)\n",
    "        elif norm==2:\n",
    "            eta = step_size * X_pgd.grad.data / X_pgd.grad.view(X_pgd.shape[0], -1).norm(2, dim=-1)\\\n",
    "                    .view(-1, 1, 1, 1)\n",
    "            X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "            X_pgd = project(X, X_pgd, norm, epsilon)            \n",
    "        X_pgd = Variable(torch.clamp(X_pgd, 0, 1.0), requires_grad=True)\n",
    "    err_pgd = (model(X_pgd).data.max(1)[1] != y.data).float().sum()\n",
    "    return err, err_pgd\n",
    "\n",
    "def eval_adv_test_whitebox(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    evaluate model by white-box attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    robust_err_total = 0\n",
    "    natural_err_total = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # pgd attack\n",
    "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
    "        err_natural, err_robust = _pgd_whitebox(model, X, y)\n",
    "        robust_err_total += err_robust\n",
    "        natural_err_total += err_natural\n",
    "    rob = 100-robust_err_total.item()/100    \n",
    "    print('natural_acc_total: ', 100-natural_err_total.item()/100)\n",
    "    print('robust_acc_total: ', rob)\n",
    "    return rob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-INF ATENT MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training_entropy(model, optimiser, loss_fn, x, y, epoch, adversary, L, step, eps, norm):\n",
    "    \"\"\"Performs a single update against a specified adversary\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Adversial perturbation\n",
    "    alpha=0.9\n",
    "    loss = 0\n",
    "    gamma = 0\n",
    "    projector = True #(project after each inner epoch)\n",
    "        \n",
    "    for l in range(L):     \n",
    "        \n",
    "        if l==0: ## initialize using random perturbation of true x, run for one epoch\n",
    "            k=1\n",
    "            random=True\n",
    "            xp = None\n",
    "        elif l>0 and l<L-1: ## initialize with previous iterate of adversarial perturbation, run one epoch\n",
    "            k=1\n",
    "            random=False\n",
    "            xp=x_adv\n",
    "        elif l == L-1: ## initialize with previous iterate, run one epoch, project to epsilon ball\n",
    "            k=1\n",
    "            random=False\n",
    "            xp = x_adv\n",
    "          \n",
    "        x_adv,bfl = adversary(model, x, y, loss_fn, xp=xp, step=step, eps=eps, norm=norm, random=random, ep=1e-4,projector=projector,gamma=gamma)\n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        y_pred = model(x_adv)\n",
    "        pred = y_pred.max(1, keepdim=True)[1]\n",
    "        correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "        loss = (1-alpha)*loss + alpha*loss_fn(y_pred, y)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZE NET OR LOAD FROM CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural_acc_total:  99.45\n",
      "robust_acc_total:  96.4\n"
     ]
    }
   ],
   "source": [
    "model_ATENT = Net().to(DEVICE)\n",
    "#model_ATENT = nn.DataParallel(model_ATENT)\n",
    "#load pretrained state dict here\n",
    "if args['pretrained']:\n",
    "    pathstr = '../trainedmodels/MNIST/BEST_model-nn-epoch23-robacc96.pt' \n",
    "    model_ATENT.load_state_dict(torch.load(pathstr))\n",
    "    rob = eval_adv_test_whitebox(model_ATENT, DEVICE, val_loader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not args['pretrained']:\n",
    "    ## training params\n",
    "    epochs = args['epochs']  \n",
    "    lr_init = args['lr']\n",
    "    optimizer = optim.SGD(model_ATENT.parameters(), lr=lr_init, momentum=0.9)\n",
    "    ## train model\n",
    "\n",
    "    for epoch in range(41, epochs+1):\n",
    "        print('Epoch:',epoch)\n",
    "\n",
    "        # adversarial training\n",
    "        train_adversarial(adversarial_training_entropy,model_ATENT, DEVICE, train_loader, optimizer, epoch,adversary=entropySmoothing,L=2*args['num_steps'],step=args['step_size'],eps=args['epsilon'],norm=args['norm'])\n",
    "\n",
    "        # evaluation on natural and adversarial examples\n",
    "        print('================================================================')\n",
    "        eval_train(model_ATENT, DEVICE, train_loader)\n",
    "        rob = eval_adv_test_whitebox(model_ATENT, DEVICE, val_loader)            \n",
    "        print('================================================================')\n",
    "\n",
    "        # save checkpoint\n",
    "        if (epoch-1) % 1 == 0:\n",
    "            torch.save(model_ATENT.state_dict(),\n",
    "                       os.path.join(model_dir, 'model-nn-epoch{}-robacc{}.pt'.format(epoch,int(np.round(rob)))))\n",
    "\n",
    "    ## save model\n",
    "    modelname = '../trainedmodels/'+dataset+'/'+NetName+'_ATENT_'+args['attack']+'_ep'+str(epochs)+'_lr'+str(lr_init)+'.pt'\n",
    "    #torch.save(model_ATENT,modelname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
