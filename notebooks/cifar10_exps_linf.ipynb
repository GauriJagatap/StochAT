{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main script for training a classifier for CIFAR-10 using l_inf ATENT [Table 3 and Figure 3 of paper].\n",
    "\n",
    "Notebook contains printed result from evaluation of pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Training via ENTropic regularization (l_inf ATENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoTA - collected from various papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanila SGD: \n",
    "MNIST - 99%+ (most cnns), CIFAR10 - 93%+ (resnet18), 96%+ (wideresnet) \n",
    "\n",
    "MNIST:\n",
    "\n",
    "adversarial attacks: \n",
    "l-inf @ eps = 80/255 PGD @20 steps: TRADES - 96.07%, MART 96.48%, MMA 95.25%, PGD - 96.01% (7 layer cnn)\n",
    "\n",
    "adversarial attacks:\n",
    "l-2 @ eps = 2 PGD @40 steps: MMA - 73.02\n",
    "\n",
    "CIFAR10:\n",
    "\n",
    "adversarial attacks: \n",
    "l-inf @ eps = 8/255 PGD @20 steps: \n",
    "(WideResNet) TRADES 53-56% - (WRN-34-10), MART 57-58% (WRN-34-10), MMA 47%, PGD 48% - (WRN-32-10)// 49% - (WRN-34-10), Std - 0.03%\n",
    "\n",
    "Benign accuracies: \n",
    "(WideResNet)TRADES 84.92%, MART 83.62%, MMA 84.36, PGD 87.14%, Std 95.8% \n",
    "\n",
    "adversarial attacks: \n",
    "l-inf @ eps = 8/255 @20 steps: \n",
    "(ResNet10) TRADES 45.4%, MART 46.6%, MMA 37.26%, PGD 42.27%, Std 0.14%\n",
    "\n",
    "adversarial attacks:\n",
    "l-2 @ eps = 128/255: MMA 67%, PGD 68%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference repos for baselines:\n",
    "TRADES :\n",
    "https://github.com/yaodongyu/TRADES (MNIST: small cnn, CIFAR10: WideResNet34)\n",
    "MMA : \n",
    "https://github.com/BorealisAI/mma_training (MNIST: lenet5, CIFAR10: WideResNet28)\n",
    "MART :\n",
    " https://github.com/YisenWang/MART (CIFAR10: ResNet18 and WideResNet34)\n",
    "PGD: (CIFAR10: ResNet50) https://github.com/MadryLab/robustness "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append('../adversarial/')\n",
    "sys.path.append('../architectures/')\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functional import entropySmoothing\n",
    "from utils import eval_train, eval_test, infnorm, train_adversarial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET TRAINING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "#data loading\n",
    "args['seed'] = 1\n",
    "args['test_batch_size'] = 128\n",
    "args['train_batch_size'] = 128\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "args['no_cuda'] = False\n",
    "\n",
    "if not args['no_cuda']:\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = 'cuda'\n",
    "    else:\n",
    "        DEVICE = 'cpu'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# params for SGLD (inner loop)\n",
    "args['attack'] = 'l_inf'\n",
    "args['norm'] = 'inf'\n",
    "args['epsilon'] = 0.031\n",
    "args['num_steps'] = 10\n",
    "args['step_size'] = 0.007\n",
    "args['random'] =True\n",
    "\n",
    "# params for SGD (outer loop)\n",
    "args['lr'] = 0.1\n",
    "args['momentum'] = 0.9\n",
    "args['weight_decay'] = 5e-4\n",
    "args['epochs'] = 76\n",
    "args['save_freq'] = 1\n",
    "\n",
    "# load model\n",
    "args['pretrained'] = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = 'CIFAR10' # [MNIST, CIFAR10]\n",
    "\n",
    "# setup data loader\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train = datasets.CIFAR10('../../data/CIFAR10', train=True, transform=transform_train, download=True)\n",
    "val = datasets.CIFAR10('../../data/CIFAR10', train=False, transform=transform_test, download=True)\n",
    "    \n",
    "train_loader = DataLoader(train, batch_size=args['test_batch_size'], shuffle=True, **kwargs)\n",
    "val_loader = DataLoader(val, batch_size=args['train_batch_size'], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='CIFAR10':\n",
    "    #[ResNet18,ResNet34,ResNet50,WideResNet]\n",
    "    from resnet import ResNet18,ResNet34,ResNet50\n",
    "    from wideresnet import WideResNet\n",
    "    Net = WideResNet\n",
    "    NetName = 'WideResNet34'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET RANDOM SEED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f540c05ff10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_num_threads(2)\n",
    "if DEVICE=='cuda':\n",
    "    torch.cuda.set_device(-1)\n",
    "    torch.cuda.manual_seed(args['seed'])\n",
    "    cudnn.benchmark = True\n",
    "random.seed(args['seed'])\n",
    "np.random.seed(args['seed'])\n",
    "torch.manual_seed(args['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHITEBOX L-INF ATTACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pgd_whitebox(model,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=args['epsilon'],\n",
    "                  num_steps=20,\n",
    "                  step_size=0.003\n",
    "                 ):\n",
    "    out = model(X)\n",
    "    err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "    if args['random']:\n",
    "        random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(DEVICE)\n",
    "        X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            loss = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "        loss.backward()\n",
    "        eta = step_size * X_pgd.grad.data.sign()\n",
    "        X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "        eta = torch.clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "        X_pgd = Variable(X.data + eta, requires_grad=True)\n",
    "        X_pgd = Variable(torch.clamp(X_pgd, 0, 1.0), requires_grad=True)\n",
    "    err_pgd = (model(X_pgd).data.max(1)[1] != y.data).float().sum()\n",
    "    with torch.no_grad():\n",
    "        loss_pgd = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "    return err, err_pgd, loss_pgd.item()\n",
    "\n",
    "def eval_adv_test_whitebox(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    evaluate model by white-box attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    robust_err_total = 0\n",
    "    natural_err_total = 0\n",
    "    lossrob = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # pgd attack\n",
    "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
    "        err_natural, err_robust, losspgd = _pgd_whitebox(model, X, y)\n",
    "        robust_err_total += err_robust\n",
    "        natural_err_total += err_natural\n",
    "        lossrob = lossrob + losspgd\n",
    "    rob = 100-100*robust_err_total.item()/len(test_loader.dataset)   \n",
    "    lossrob /= len(test_loader)\n",
    "    print('robust test loss:',lossrob)\n",
    "    print('natural_acc_total: ', 100-100*natural_err_total.item()/len(test_loader.dataset))\n",
    "    print('robust_acc_total: ', rob)    \n",
    "    return rob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-INF ATENT MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training_entropy(model, optimiser, loss_fn, x, y, epoch, adversary, L, step, eps, norm):\n",
    "    model.train()\n",
    "    \n",
    "    # Adversial perturbation\n",
    "    alpha=0.9\n",
    "    loss = 0\n",
    "    \n",
    "    for l in range(L):     \n",
    "        \n",
    "        if l==0: # initialize using random perturbation of true x, run for one epoch\n",
    "            random=True\n",
    "            xp = None\n",
    "            projector=False\n",
    "        elif l>0 and l<L-1: # initialize with previous iterate of adversarial perturbation, run one epoch\n",
    "            random=False\n",
    "            xp=x_adv\n",
    "            projector = False\n",
    "        elif l == L-1: # initialize with previous iterate, run one epoch, project to epsilon ball\n",
    "            random=False\n",
    "            xp = x_adv\n",
    "            projector=True\n",
    "            \n",
    "        x_adv,bfl = adversary(model, x, y, loss_fn, xp=xp, step=step, eps=eps, norm=norm, random=random, ep=1e-3,projector=projector)\n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        y_pred = model(x_adv)\n",
    "        pred = y_pred.max(1, keepdim=True)[1]\n",
    "        correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "        loss = (1-alpha)*loss + alpha*loss_fn(y_pred, y)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZE NET OR LOAD FROM CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robust test loss: 1.137818789180321\n",
      "natural_acc_total:  85.67\n",
      "robust_acc_total:  57.28\n"
     ]
    }
   ],
   "source": [
    "model_ATENT = Net().to(DEVICE)\n",
    "model_ATENT = nn.DataParallel(model_ATENT)\n",
    "#load pretrained state dict here\n",
    "if args['pretrained']:\n",
    "    pathstr = '../trainedmodels/CIFAR10/BEST_model-nn-epoch76-robacc57.pt \n",
    "    model_ATENT.load_state_dict(torch.load(pathstr))\n",
    "    rob = eval_adv_test_whitebox(model_ATENT, DEVICE, val_loader)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADJUST LEARNING RATE SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch,lr_init):\n",
    "    \"\"\"decrease the learning rate\"\"\"\n",
    "    lr = lr_init\n",
    "    if epoch >= 75:\n",
    "        lr = lr_init * 0.1 \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATHS TO STORE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../trainedmodels'): #path to save final trained model\n",
    "    os.makedirs('../trainedmodels')\n",
    "model_dir = '../trainedmodels/'+NetName #path to save each epoch while training\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN MODULE USING ATENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not args['pretrained']:    \n",
    "    ## training params\n",
    "    lr_init = args['lr']\n",
    "    optimizer = optim.SGD(model_ATENT.parameters(), lr=lr_init, momentum=args['momentum'], \n",
    "                          weight_decay=args['weight_decay'] )\n",
    "\n",
    "    ## train model\n",
    "    for epoch in range(1, args['epochs']+1):\n",
    "        print('Epoch:',epoch)\n",
    "\n",
    "        # adjust learning rate for SGD\n",
    "        adjust_learning_rate(optimizer, epoch,lr_init)\n",
    "\n",
    "        # adversarial training\n",
    "        train_adversarial(adversarial_training_entropy,model_ATENT, DEVICE, train_loader, optimizer, \n",
    "                          epoch,adversary=entropySmoothing,L=args['num_steps'],step=args['step_size'],\n",
    "                          eps=args['epsilon'],norm=args['norm'])\n",
    "\n",
    "        # evaluation on natural and adversarial examples\n",
    "        print('================================================================')\n",
    "        eval_train(model_ATENT, DEVICE, train_loader)\n",
    "        rob = eval_adv_test_whitebox(model_ATENT, DEVICE, val_loader)            \n",
    "        print('================================================================')\n",
    "\n",
    "        # save checkpoint\n",
    "        if (epoch-1) % args['save_freq'] == 0:\n",
    "            torch.save(model_ATENT.state_dict(),\n",
    "                       os.path.join(model_dir, 'model-nn-epoch{}-robacc{}.pt'.format(epoch,int(np.round(rob)))))\n",
    "\n",
    "    # save final model\n",
    "    modelname = '../trainedmodels/'+dataset+'/'+NetName+'_ATENTInf_ep'+str(epochs)+'_lr'+str(lr_init)+'_robacc_'+str(int(np.round(rob)))+'.pt'\n",
    "    torch.save(model_ATENT,modelname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
