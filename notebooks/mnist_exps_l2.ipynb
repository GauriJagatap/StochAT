{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main script for training a classifier for MNIST using l_2 ATENT [Table 1 of paper]\n",
    "\n",
    "Notebook contains printed result from evaluation of pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Training via ENTropic regularization (l_2 ATENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanila SGD: \n",
    "MNIST - 99%+ (most cnns), CIFAR10 - 93%+ (resnet18), 96%+ (wideresnet) \n",
    "\n",
    "MNIST:\n",
    "\n",
    "adversarial attacks: \n",
    "l-inf @ eps = 80/255 @20 steps: TRADES - 96.07% - (4 layer cnn), MART 96.4%, MMA 95.5%, PGD - 96.01% - (4 layer cnn)\n",
    "\n",
    "l-2 @ eps = 2 PGD @40 steps: MMA - 73.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference repos for baselines: TRADES : https://github.com/yaodongyu/TRADES (MNIST: small cnn, CIFAR10: WideResNet34) MMA : https://github.com/BorealisAI/mma_training (MNIST: lenet5, CIFAR10: WideResNet28) MART : https://github.com/YisenWang/MART (CIFAR10: ResNet18 and WideResNet34) PGD: (CIFAR10: ResNet50) https://github.com/MadryLab/robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append('../adversarial/')\n",
    "sys.path.append('../architectures/')\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functional import entropySmoothing\n",
    "from utils import eval_train, eval_test, infnorm, train_adversarial, project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET TRAINING PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "#data loading\n",
    "args['seed'] = 2\n",
    "args['test_batch_size'] = 256\n",
    "args['train_batch_size'] = 50\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "args['no_cuda'] = False\n",
    "\n",
    "if not args['no_cuda']:\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = 'cuda'\n",
    "    else:\n",
    "        DEVICE = 'cpu'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# params for SGLD (inner loop)\n",
    "args['attack'] = 'l_2'\n",
    "args['norm'] = 2\n",
    "args['epsilon'] = 2\n",
    "args['num_steps'] = 40\n",
    "args['step_size'] = 2.5 * args['epsilon']/args['num_steps']\n",
    "args['random'] = True\n",
    "\n",
    "# params for SGD (outer loop)\n",
    "args['lr'] = 0.01\n",
    "args['momentum'] = 0.9\n",
    "args['weight_decay'] = 1e-4\n",
    "args['epochs'] = 30\n",
    "args['save_freq'] = 30\n",
    "\n",
    "# load model\n",
    "args['pretrained'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'MNIST' # [MNIST, CIFAR10]\n",
    "transform = transforms.Compose([\n",
    "transforms.ToTensor()])\n",
    "train = datasets.MNIST('../../data/', train=True, transform=transform, download=True)\n",
    "val = datasets.MNIST('../../data/', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(train, batch_size=args['train_batch_size'], shuffle=True, **kwargs)\n",
    "val_loader = DataLoader(val, batch_size=args['test_batch_size'], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from advertorch_examples.models import LeNet5Madry\n",
    "Net = LeNet5Madry\n",
    "NetName = 'LeNet5Madry'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM SEED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1750ccc070>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = args['seed']\n",
    "torch.set_num_threads(2)\n",
    "if DEVICE=='cuda':\n",
    "    torch.cuda.set_device(-1)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    cudnn.benchmark = True\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pgd_whitebox(model,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=args['epsilon'],\n",
    "                  norm=args['norm'],\n",
    "                  num_steps=args['num_steps'],\n",
    "                  step_size=args['step_size']):\n",
    "    out = model(X)\n",
    "    err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "    if args['random']:\n",
    "        random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(DEVICE)\n",
    "        X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            loss = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "        loss.backward()\n",
    "        if norm=='inf':\n",
    "            eta = step_size * X_pgd.grad.data.sign()\n",
    "            X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "            eta = torch.clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "            X_pgd = Variable(X.data + eta, requires_grad=True)\n",
    "        elif norm==2:\n",
    "            eta = step_size * X_pgd.grad.data / X_pgd.grad.view(X_pgd.shape[0], -1).norm(2, dim=-1)\\\n",
    "                    .view(-1, 1, 1, 1)\n",
    "            X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "            X_pgd = project(X, X_pgd, norm, epsilon)            \n",
    "        X_pgd = Variable(torch.clamp(X_pgd, 0, 1.0), requires_grad=True)\n",
    "    err_pgd = (model(X_pgd).data.max(1)[1] != y.data).float().sum()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_pgd = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "    return err, err_pgd, loss_pgd.item()\n",
    "\n",
    "def eval_adv_test_whitebox(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    evaluate model by white-box attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    robust_err_total = 0\n",
    "    natural_err_total = 0\n",
    "    lossrob  = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # pgd attack\n",
    "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
    "        err_natural, err_robust, losspgd = _pgd_whitebox(model, X, y)\n",
    "        robust_err_total += err_robust\n",
    "        natural_err_total += err_natural\n",
    "        lossrob = lossrob + losspgd\n",
    "    rob = 100-100*robust_err_total.item()/len(test_loader.dataset)   \n",
    "    lossrob /= len(test_loader)\n",
    "    print('robust test loss:',lossrob)\n",
    "    print('natural_acc_total: ', 100-100*natural_err_total.item()/len(test_loader.dataset))\n",
    "    print('robust_acc_total: ', rob)\n",
    "    return rob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l-2 ATENT MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training_entropy(model, optimiser, loss_fn, x, y, epoch, adversary, L, step, eps, norm):\n",
    "    \"\"\"Performs a single update against a specified adversary\"\"\"\n",
    "    model.train()\n",
    "    # Adversial perturbation\n",
    "    alpha=0.9\n",
    "    loss = 0\n",
    "    gamma = 5e-2\n",
    "    projector = True\n",
    "    \n",
    "    for l in range(L):     \n",
    "        \n",
    "        if l==0: ## initialize using random perturbation of true x, run for one epoch\n",
    "            k=1\n",
    "            random=True\n",
    "            xp = None\n",
    "            projector=True\n",
    "        elif l>0 and l<L-1: ## initialize with previous iterate of adversarial perturbation, run one epoch\n",
    "            k=1\n",
    "            random=False\n",
    "            xp=x_adv\n",
    "            projector = True\n",
    "        elif l == L-1: ## initialize with previous iterate, run one epoch, project to epsilon ball\n",
    "            k=1\n",
    "            random=False\n",
    "            xp = x_adv\n",
    "            if norm==2:\n",
    "                projector=True\n",
    "            elif norm=='inf':    \n",
    "                projector=True\n",
    "            \n",
    "        x_adv,bfl = adversary(model, x, y, loss_fn, xp=xp, step=step, eps=eps, norm=norm, random=random, ep=1e-2,projector=projector,gamma=gamma, debug=False)\n",
    "        optimiser.zero_grad()\n",
    "        y_pred = model(x_adv)\n",
    "        pred = y_pred.max(1, keepdim=True)[1]\n",
    "        correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "        \n",
    "        loss_lg = loss_fn(y_pred, y)\n",
    "        loss = (1-alpha)*loss + alpha*loss_lg\n",
    "     \n",
    "        if bfl:\n",
    "            break      \n",
    "        \n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZE NET OR LOAD FROM CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robust test loss: 0.4925464943051338\n",
      "natural_acc_total:  98.66\n",
      "robust_acc_total:  79.3\n"
     ]
    }
   ],
   "source": [
    "model_ATENT = Net().to(DEVICE)\n",
    "#model_ATENT = nn.DataParallel(model_ATENT)\n",
    "#load pretrained state dict here\n",
    "if args['pretrained']:\n",
    "    pathstr = '../trainedmodels/MNIST/BEST-model-nn-epoch30-robacc77.pt'\n",
    "    model_ATENT.load_state_dict(torch.load(pathstr))\n",
    "    rob = eval_adv_test_whitebox(model_ATENT, DEVICE, val_loader)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADJUST LEARNING RATE SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch,lr_init):\n",
    "    \"\"\"decrease the learning rate\"\"\"\n",
    "    lr = lr_init\n",
    "    if epoch >= 25:   \n",
    "        lr = lr_init * 0.1\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATHS TO STORE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../trainedmodels'): #path to save final trained model\n",
    "    os.makedirs('../trainedmodels')\n",
    "model_dir = '../trainedmodels/'+NetName #path to save each epoch while training\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN MODULE USING ATENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not args['pretrained']:    \n",
    "    ## training params\n",
    "    epochs = args['epochs']  \n",
    "    lr_init = args['lr']\n",
    "    #optimizer = optim.Adam(model_ATENT.parameters(), lr=lr_init)\n",
    "\n",
    "    optimizer = optim.SGD(model_ATENT.parameters(), lr=lr_init, momentum=0.9)\n",
    "    ## train model\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print('Epoch:',epoch)\n",
    "\n",
    "        # adjust learning rate for SGD\n",
    "        #adjust_learning_rate(optimizer, epoch,lr_init)\n",
    "\n",
    "        # adversarial training\n",
    "        train_adversarial(adversarial_training_entropy,model_ATENT, DEVICE, \n",
    "                          train_loader, optimizer, epoch,adversary=entropySmoothing,\n",
    "                          L=args['num_steps'],step=args['step_size'],eps=args['epsilon'],norm=args['norm'])\n",
    "\n",
    "        # evaluation on natural and adversarial examples\n",
    "        print('================================================================')\n",
    "        eval_train(model_ATENT, DEVICE, train_loader)\n",
    "        rob = eval_adv_test_whitebox(model_ATENT, DEVICE, val_loader)            \n",
    "        print('================================================================')\n",
    "\n",
    "        # save checkpoint\n",
    "        if (epoch-1) % 1 == 0:\n",
    "            torch.save(model_ATENT.state_dict(),\n",
    "                       os.path.join(model_dir, 'model-nn-epoch{}-robacc{}.pt'.format(epoch,int(np.round(rob)))))\n",
    "\n",
    "    ## save model\n",
    "\n",
    "    modelname = '../trainedmodels/'+dataset+'/'+NetName+'_ATENT_'+args['attack']+'_ep'+str(epochs)+'_lr'+str(lr_init)+'.pt'\n",
    "    torch.save(model_ATENT,modelname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
