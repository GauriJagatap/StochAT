{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Adversarial Training (StochAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanila SGD: \n",
    "MNIST - 99%+ (most cnns), CIFAR10 - 93%+ (resnet18), 96%+ (wideresnet) \n",
    "\n",
    "MNIST:\n",
    "\n",
    "adversarial attacks: \n",
    "l-inf @ eps = 80/255 @20 steps: TRADES - 96.07% - (4 layer cnn), MART 96.4%, MMA 95.5%, PGD - 96.01% - (4 layer cnn)\n",
    "\n",
    "adversarial attacks:\n",
    "l-2 @ eps = 32/255 (check): TRADES, MMA, PGD\n",
    "\n",
    "CIFAR10:\n",
    "\n",
    "adversarial attacks: \n",
    "l-inf @ eps = 8/255 @20 steps: \n",
    "TRADES 53-56% - (WRN-34-10), MART 57-58% (WRN-34-10), MMA 47%, PGD 48% - (WRN-32-10)// 49% - (WRN-34-10), Std - 0.03%\n",
    "https://openreview.net/pdf?id=rklOg6EFwS (Table 4)\n",
    "\n",
    "adversarial attacks: \n",
    "l-inf @ eps = 8/255 @20 steps: \n",
    "[ResNet10] TRADES 45.4%, MART 46.6%, MMA 37.26%, PGD 42.27%, Std 0.14%\n",
    "\n",
    "Benign accuracies: TRADES 84.92%, MART 83.62%, MMA 84.36, PGD 87.14%, Std 95.8% [wideresnet]\n",
    "https://openreview.net/pdf?id=Ms9zjhVB5R (Table 1)\n",
    "\n",
    "adversarial attacks:\n",
    "l-2 @ eps = 32/255 (check): TRADES, MART, MMA, PGD\n",
    "\n",
    "TBD: CWinf attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained models for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download pretrained models and place in ../trainedmodels/MNIST or ../trainedmodels/CIFAR10 respectively\n",
    "\n",
    "### TRADES :\n",
    "https://github.com/yaodongyu/TRADES (MNIST: small cnn, CIFAR10: WideResNet34)\n",
    "### MMA : \n",
    "https://github.com/BorealisAI/mma_training (MNIST: lenet5, CIFAR10: WideResNet28)\n",
    "### MART :\n",
    " https://github.com/YisenWang/MART (CIFAR10: ResNet18 and WideResNet34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from multiprocessing import cpu_count\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import olympic\n",
    "from typing import Union, Callable, Tuple\n",
    "import sys\n",
    "sys.path.append('../adversarial/')\n",
    "sys.path.append('../architectures/')\n",
    "from functional import boundary, iterated_fgsm, local_search, pgd, entropySmoothing\n",
    "from ESGD_utils import *\n",
    "import pickle\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import argparse, math, random\n",
    "import ESGD_optim\n",
    "from trades import trades_loss\n",
    "import advertorch\n",
    "from torch.autograd import Variable\n",
    "from utils import project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place data folders outside working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 4, 'pin_memory': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['test_batch_size'] = 128\n",
    "args['no_cuda'] = False\n",
    "args['epsilon'] = 2\n",
    "args['num_steps'] = 40\n",
    "args['step_size'] = 0.25#0.125#2.5*args['epsilon']/args['num_steps']\n",
    "args['random'] =True,\n",
    "args['white_box_attack']=True\n",
    "args['log_interval'] = 100\n",
    "args['beta'] = 1\n",
    "args['seed'] = 2\n",
    "args['lr'] = 0.001\n",
    "args['momentum'] = 0.9\n",
    "args['batch_size'] = 50\n",
    "args['attack'] = 'l_2'\n",
    "args['norm'] = 2\n",
    "args['epochs'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'MNIST' # [MNIST, CIFAR10]\n",
    "#if dataset == 'MNIST':\n",
    "transform = transforms.Compose([\n",
    "transforms.ToTensor()])\n",
    "train = datasets.MNIST('../../data/', train=True, transform=transform, download=True)\n",
    "val = datasets.MNIST('../../data/', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(train, batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
    "val_loader = DataLoader(val, batch_size=args['test_batch_size'], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALIZE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if dataset=='MNIST':\n",
    "from advertorch_examples.models import LeNet5Madry\n",
    "Net = LeNet5Madry\n",
    "NetName = 'LeNet5Madry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "advertorch_examples.models.LeNet5Madry"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM SEED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9b2d3be110>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = args['seed']\n",
    "torch.set_num_threads(2)\n",
    "if DEVICE=='cuda':\n",
    "    torch.cuda.set_device(-1)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    cudnn.benchmark = True\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_train(model, device, train_loader):\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            train_loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('Training: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "    training_accuracy = correct / len(train_loader.dataset)\n",
    "    return train_loss, training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pgd_whitebox(model,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=2,#args['epsilon'],\n",
    "                  norm=args['norm'],\n",
    "                  num_steps=args['num_steps'],\n",
    "                  step_size=args['step_size']):\n",
    "    out = model(X)\n",
    "    err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "    if args['random']:\n",
    "        random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(DEVICE)\n",
    "        X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            loss = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "        loss.backward()\n",
    "        if norm=='inf':\n",
    "            eta = step_size * X_pgd.grad.data.sign()\n",
    "            X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "            eta = torch.clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "            X_pgd = Variable(X.data + eta, requires_grad=True)\n",
    "        elif norm==2:\n",
    "            #print('l2 attack')\n",
    "            eta = step_size * X_pgd.grad.data / X_pgd.grad.view(X_pgd.shape[0], -1).norm(2, dim=-1)\\\n",
    "                    .view(-1, 1, 1, 1)\n",
    "            X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "            X_pgd = project(X, X_pgd, norm, epsilon)            \n",
    "        X_pgd = Variable(torch.clamp(X_pgd, 0, 1.0), requires_grad=True)\n",
    "        #print('distance of attack:',torch.norm(X_pgd-X)/np.sqrt(128))\n",
    "    err_pgd = (model(X_pgd).data.max(1)[1] != y.data).float().sum()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_pgd = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "    #print('err pgd (white-box): ', err_pgd)\n",
    "    return err, err_pgd, loss_pgd.item()\n",
    "\n",
    "def eval_adv_test_whitebox(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    evaluate model by white-box attack\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    robust_err_total = 0\n",
    "    natural_err_total = 0\n",
    "    lossrob  = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # pgd attack\n",
    "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
    "        err_natural, err_robust, losspgd = _pgd_whitebox(model, X, y)\n",
    "        robust_err_total += err_robust\n",
    "        natural_err_total += err_natural\n",
    "        lossrob = lossrob + losspgd\n",
    "    rob = 100-100*robust_err_total.item()/len(test_loader.dataset)   \n",
    "    lossrob /= len(test_loader)\n",
    "    print('robust test loss:',lossrob)\n",
    "    print('natural_acc_total: ', 100-100*natural_err_total.item()/len(test_loader.dataset))\n",
    "    print('robust_acc_total: ', rob)\n",
    "    return rob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../trainedmodels'):\n",
    "    os.makedirs('../trainedmodels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infnorm(x):\n",
    "    infn = torch.max(torch.abs(x.detach().cpu()))\n",
    "    return infn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pgd_white(model,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=2,#args['epsilon'],\n",
    "                  norm=args['norm'],\n",
    "                  num_steps=args['num_steps'],\n",
    "                  step_size=args['step_size']):\n",
    "    out = model(X)\n",
    "    err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "    if args['random']:\n",
    "        random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(DEVICE)\n",
    "        X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            loss = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "        loss.backward()\n",
    "        if norm=='inf':\n",
    "            eta = step_size * X_pgd.grad.data.sign()\n",
    "            X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "            eta = torch.clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "            X_pgd = Variable(X.data + eta, requires_grad=True)\n",
    "        elif norm==2:\n",
    "            #print('l2 attack')\n",
    "            eta = step_size * X_pgd.grad.data / X_pgd.grad.view(X_pgd.shape[0], -1).norm(2, dim=-1)\\\n",
    "                    .view(-1, 1, 1, 1)\n",
    "            X_pgd = Variable(X_pgd.data + eta, requires_grad=True)          \n",
    "            X_pgd = project(X, X_pgd, norm, epsilon)            \n",
    "        X_pgd = Variable(torch.clamp(X_pgd, 0, 1.0), requires_grad=True)\n",
    "        #print('distance of attack:',torch.norm(X_pgd-X)/np.sqrt(128))\n",
    "    err_pgd = (model(X_pgd).data.max(1)[1] != y.data).float().sum()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_pgd = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "    #print('err pgd (white-box): ', err_pgd)\n",
    "    return X_pgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial(method,model, device, train_loader, optimizer, epoch,adversary,L,step,eps,norm):\n",
    "    totalcorrect = 0\n",
    "    totalloss = 0\n",
    "    batches = len(train_loader)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #print('batch:{:d}/{:d}'.format(batch_idx,batches))\n",
    "        data, target = data.to(device), target.to(device)        \n",
    "        sgd_loss = nn.CrossEntropyLoss()\n",
    "        # calculate robust loss per batch\n",
    "        loss, correct = method(model,optimizer,sgd_loss,data,target,epoch,adversary,L,step,eps,norm)\n",
    "        totalcorrect += correct\n",
    "        totalloss += loss\n",
    "    totalloss /= batches\n",
    "    print('robust train accuracy:',100*totalcorrect/len(train_loader.dataset),'rob train loss:',totalloss.item())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL USING SAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training(model, optimiser, loss_fn, x, y, epoch, adversary, k, step, eps, norm):\n",
    "    \"\"\"Performs a single update against a specified adversary\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Adversial perturbation\n",
    "    x_adv = _pgd_white(model, x, y)\n",
    "    #adversary(model, x, y, loss_fn, k=k, step=step, eps=eps, norm=norm, random=True)\n",
    "    #print(torch.norm(x-x_adv)/np.sqrt(args['batch_size']))\n",
    "    optimiser.zero_grad()\n",
    "    y_pred = model(x_adv)\n",
    "    pred = y_pred.max(1, keepdim=True)[1]\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    #print('% correct:',100*correct/args['batch_size'])\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training_entropy(model, optimiser, loss_fn, x, y, epoch, adversary, L, step, eps, norm):\n",
    "    \"\"\"Performs a single update against a specified adversary\"\"\"\n",
    "    model.train()\n",
    "    # Adversial perturbation\n",
    "    alpha=0.95\n",
    "    loss = 0\n",
    "    if norm==2:\n",
    "        gamma = 1e-2\n",
    "    elif norm=='inf':\n",
    "        gamma = 0\n",
    "        \n",
    "    for l in range(L):     \n",
    "        \n",
    "        if l==0: ## initialize using random perturbation of true x, run for one epoch\n",
    "            k=1\n",
    "            random=True\n",
    "            xp = None\n",
    "            projector=True\n",
    "        elif l>0 and l<L-1: ## initialize with previous iterate of adversarial perturbation, run one epoch\n",
    "            k=1\n",
    "            random=False\n",
    "            xp=x_adv\n",
    "            projector = True\n",
    "        elif l == L-1: ## initialize with previous iterate, run one epoch, project to epsilon ball\n",
    "            k=1\n",
    "            random=False\n",
    "            xp = x_adv\n",
    "            if norm==2:\n",
    "                projector=True\n",
    "            elif norm=='inf':    \n",
    "                projector=True\n",
    "            \n",
    "        x_adv,bfl = adversary(model, x, y, loss_fn, xp=xp, step=step, eps=eps, norm=norm, random=random, ep=1e-2,projector=projector,gamma=gamma, debug=False)\n",
    "        optimiser.zero_grad()\n",
    "        y_pred = model(x_adv)\n",
    "        pred = y_pred.max(1, keepdim=True)[1]\n",
    "        correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "        \n",
    "        loss_lg = loss_fn(y_pred, y)\n",
    "        #print(loss_lg)\n",
    "        #print('langevin iter:',l,'loss langevin:',loss_lg.item())\n",
    "        loss = (1-alpha)*loss + alpha*loss_lg\n",
    "        #print('adv dist:',torch.norm(x_adv-x)/np.sqrt(128))\n",
    "        \n",
    "        if bfl:\n",
    "            break\n",
    "        \n",
    "        \n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../SmallCNN_MNIST_l2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch,lr_init):\n",
    "    \"\"\"decrease the learning rate\"\"\"\n",
    "    lr = lr_init\n",
    "    if epoch >= 15:   \n",
    "        lr = lr_init * 0.2\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "robust train accuracy: 35.895 rob train loss: 1.8094295263290405\n",
      "================================================================\n",
      "Training: Average loss: 0.4386, Accuracy: 55086/60000 (92%)\n",
      "Test: Average loss: 0.4245, Accuracy: 9260/10000 (92.60%)\n",
      "robust test loss: 1.10993891954422\n",
      "natural_acc_total:  92.6\n",
      "robust_acc_total:  57.41\n",
      "================================================================\n",
      "Epoch: 2\n",
      "robust train accuracy: 63.01166666666666 rob train loss: 0.9335871338844299\n",
      "================================================================\n",
      "Training: Average loss: 0.2153, Accuracy: 57587/60000 (96%)\n",
      "Test: Average loss: 0.2062, Accuracy: 9620/10000 (96.20%)\n",
      "robust test loss: 0.8249184934398796\n",
      "natural_acc_total:  96.2\n",
      "robust_acc_total:  66.47\n",
      "================================================================\n",
      "Epoch: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-acc4f1033f14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# adversarial training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_adversarial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madversarial_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_PGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madversary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epsilon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# evaluation on natural and adversarial examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-99f601168100>\u001b[0m in \u001b[0;36mtrain_adversarial\u001b[0;34m(method, model, device, train_loader, optimizer, epoch, adversary, L, step, eps, norm)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msgd_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# calculate robust loss per batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msgd_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madversary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtotalcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotalloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-5c879083d5e7>\u001b[0m in \u001b[0;36madversarial_training\u001b[0;34m(model, optimiser, loss_fn, x, y, epoch, adversary, k, step, eps, norm)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Adversial perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mx_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pgd_white\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#adversary(model, x, y, loss_fn, k=k, step=step, eps=eps, norm=norm, random=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print(torch.norm(x-x_adv)/np.sqrt(args['batch_size']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-bb6af75cb974>\u001b[0m in \u001b[0;36m_pgd_white\u001b[0;34m(model, X, y, epsilon, norm, num_steps, step_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mX_pgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mX_pgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_pgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mX_pgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#print('distance of attack:',torch.norm(X_pgd-X)/np.sqrt(128))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/StochAT/adversarial/utils.py\u001b[0m in \u001b[0;36mproject\u001b[0;34m(x, x_adv, norm, eps)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mscaling_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mscaling_factor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# .view() assumes batched images as a 4D Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## initialize model\n",
    "model_PGD = Net().to(DEVICE)\n",
    "## training params\n",
    "epochs = args['epochs']  \n",
    "lr_init = args['lr']\n",
    "#optimizer = optim.Adam(model_PGD.parameters(), lr=lr_init)\n",
    "\n",
    "optimizer = optim.SGD(model_PGD.parameters(), lr=lr_init, momentum=0.9)\n",
    "## train model\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print('Epoch:',epoch)\n",
    "    \n",
    "    # adjust learning rate for SGD\n",
    "    #adjust_learning_rate(optimizer, epoch,lr_init)\n",
    "    \n",
    "    # adversarial training\n",
    "    train_adversarial(adversarial_training,model_PGD, DEVICE, train_loader, optimizer, epoch,adversary=pgd,L=args['num_steps'],step=2*args['step_size'],eps=args['epsilon'],norm=args['norm'])\n",
    "\n",
    "    # evaluation on natural and adversarial examples\n",
    "    print('================================================================')\n",
    "    eval_train(model_PGD, DEVICE, train_loader)\n",
    "    eval_test(model_PGD, DEVICE, val_loader)\n",
    "\n",
    "    rob = eval_adv_test_whitebox(model_PGD, DEVICE, val_loader)            \n",
    "    print('================================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_test(model_PGD, DEVICE, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "robust train accuracy: 36.93833333333333 rob train loss: 1.7865444421768188\n",
      "================================================================\n",
      "Training: Average loss: 0.4301, Accuracy: 55327/60000 (92%)\n",
      "robust test loss: 1.0593422198597389\n",
      "natural_acc_total:  93.16\n",
      "robust_acc_total:  59.98\n",
      "================================================================\n",
      "Epoch: 2\n",
      "robust train accuracy: 61.711666666666666 rob train loss: 0.9453860521316528\n",
      "================================================================\n",
      "Training: Average loss: 0.2338, Accuracy: 57375/60000 (96%)\n",
      "robust test loss: 0.8082012462465069\n",
      "natural_acc_total:  95.84\n",
      "robust_acc_total:  67.03\n",
      "================================================================\n",
      "Epoch: 3\n",
      "robust train accuracy: 66.56666666666666 rob train loss: 0.7990458011627197\n",
      "================================================================\n",
      "Training: Average loss: 0.1883, Accuracy: 57821/60000 (96%)\n",
      "robust test loss: 0.7428850384452675\n",
      "natural_acc_total:  96.48\n",
      "robust_acc_total:  69.41\n",
      "================================================================\n",
      "Epoch: 4\n",
      "robust train accuracy: 68.81333333333333 rob train loss: 0.7327772974967957\n",
      "================================================================\n",
      "Training: Average loss: 0.1522, Accuracy: 58362/60000 (97%)\n",
      "robust test loss: 0.6808857302876967\n",
      "natural_acc_total:  97.51\n",
      "robust_acc_total:  71.45\n",
      "================================================================\n",
      "Epoch: 5\n",
      "robust train accuracy: 70.485 rob train loss: 0.6925215721130371\n",
      "================================================================\n",
      "Training: Average loss: 0.1357, Accuracy: 58642/60000 (98%)\n",
      "robust test loss: 0.6437343828663041\n",
      "natural_acc_total:  97.61\n",
      "robust_acc_total:  73.23\n",
      "================================================================\n",
      "Epoch: 6\n",
      "robust train accuracy: 71.40833333333333 rob train loss: 0.6630778312683105\n",
      "================================================================\n",
      "Training: Average loss: 0.1277, Accuracy: 58728/60000 (98%)\n",
      "robust test loss: 0.6351582011844539\n",
      "natural_acc_total:  97.79\n",
      "robust_acc_total:  73.02\n",
      "================================================================\n",
      "Epoch: 7\n",
      "robust train accuracy: 72.44 rob train loss: 0.6389316320419312\n",
      "================================================================\n",
      "Training: Average loss: 0.1107, Accuracy: 58898/60000 (98%)\n",
      "robust test loss: 0.6081709861755371\n",
      "natural_acc_total:  98.06\n",
      "robust_acc_total:  74.69\n",
      "================================================================\n",
      "Epoch: 8\n",
      "robust train accuracy: 73.12166666666667 rob train loss: 0.6202886700630188\n",
      "================================================================\n",
      "Training: Average loss: 0.1039, Accuracy: 58936/60000 (98%)\n",
      "robust test loss: 0.5927269724728186\n",
      "natural_acc_total:  98.13\n",
      "robust_acc_total:  75.22\n",
      "================================================================\n",
      "Epoch: 9\n",
      "robust train accuracy: 73.76333333333334 rob train loss: 0.605671226978302\n",
      "================================================================\n",
      "Training: Average loss: 0.1029, Accuracy: 59006/60000 (98%)\n",
      "robust test loss: 0.5806837517626678\n",
      "natural_acc_total:  98.32\n",
      "robust_acc_total:  75.36\n",
      "================================================================\n",
      "Epoch: 10\n",
      "robust train accuracy: 74.175 rob train loss: 0.5937226414680481\n",
      "================================================================\n",
      "Training: Average loss: 0.0966, Accuracy: 59039/60000 (98%)\n",
      "robust test loss: 0.5702625145263309\n",
      "natural_acc_total:  98.29\n",
      "robust_acc_total:  75.85\n",
      "================================================================\n",
      "Epoch: 11\n",
      "robust train accuracy: 74.76833333333333 rob train loss: 0.5801260471343994\n",
      "================================================================\n",
      "Training: Average loss: 0.0968, Accuracy: 58926/60000 (98%)\n",
      "robust test loss: 0.5715075280470184\n",
      "natural_acc_total:  98.12\n",
      "robust_acc_total:  75.88\n",
      "================================================================\n",
      "Epoch: 12\n",
      "robust train accuracy: 75.08833333333334 rob train loss: 0.5709062814712524\n",
      "================================================================\n",
      "Training: Average loss: 0.0914, Accuracy: 59173/60000 (99%)\n",
      "robust test loss: 0.5551971170721175\n",
      "natural_acc_total:  98.54\n",
      "robust_acc_total:  76.6\n",
      "================================================================\n",
      "Epoch: 13\n",
      "robust train accuracy: 75.35833333333333 rob train loss: 0.5615853071212769\n",
      "================================================================\n",
      "Training: Average loss: 0.0872, Accuracy: 59163/60000 (99%)\n",
      "robust test loss: 0.5468981190572811\n",
      "natural_acc_total:  98.47\n",
      "robust_acc_total:  76.59\n",
      "================================================================\n",
      "Epoch: 14\n",
      "robust train accuracy: 75.91833333333334 rob train loss: 0.551598072052002\n",
      "================================================================\n",
      "Training: Average loss: 0.0867, Accuracy: 59188/60000 (99%)\n",
      "robust test loss: 0.5493608507551725\n",
      "natural_acc_total:  98.47\n",
      "robust_acc_total:  76.46000000000001\n",
      "================================================================\n",
      "Epoch: 15\n",
      "robust train accuracy: 76.14833333333333 rob train loss: 0.5444035530090332\n",
      "================================================================\n",
      "Training: Average loss: 0.0894, Accuracy: 59206/60000 (99%)\n",
      "robust test loss: 0.5501620250789425\n",
      "natural_acc_total:  98.53\n",
      "robust_acc_total:  76.57\n",
      "================================================================\n",
      "Epoch: 16\n",
      "robust train accuracy: 76.50166666666667 rob train loss: 0.537619948387146\n",
      "================================================================\n",
      "Training: Average loss: 0.0807, Accuracy: 59229/60000 (99%)\n",
      "robust test loss: 0.5371879971102823\n",
      "natural_acc_total:  98.5\n",
      "robust_acc_total:  77.21000000000001\n",
      "================================================================\n",
      "Epoch: 17\n",
      "robust train accuracy: 76.68166666666667 rob train loss: 0.5321066975593567\n",
      "================================================================\n",
      "Training: Average loss: 0.0808, Accuracy: 59309/60000 (99%)\n",
      "robust test loss: 0.524956183342994\n",
      "natural_acc_total:  98.59\n",
      "robust_acc_total:  77.82\n",
      "================================================================\n",
      "Epoch: 18\n",
      "robust train accuracy: 77.06 rob train loss: 0.5241764783859253\n",
      "================================================================\n",
      "Training: Average loss: 0.0772, Accuracy: 59280/60000 (99%)\n",
      "robust test loss: 0.5279522302407252\n",
      "natural_acc_total:  98.59\n",
      "robust_acc_total:  77.64\n",
      "================================================================\n",
      "Epoch: 19\n",
      "robust train accuracy: 77.20833333333333 rob train loss: 0.5197961330413818\n",
      "================================================================\n",
      "Training: Average loss: 0.0787, Accuracy: 59278/60000 (99%)\n",
      "robust test loss: 0.5354123341886303\n",
      "natural_acc_total:  98.35\n",
      "robust_acc_total:  77.45\n",
      "================================================================\n",
      "Epoch: 20\n",
      "robust train accuracy: 77.36166666666666 rob train loss: 0.513713002204895\n",
      "================================================================\n",
      "Training: Average loss: 0.0753, Accuracy: 59329/60000 (99%)\n",
      "robust test loss: 0.5216861535099488\n",
      "natural_acc_total:  98.57\n",
      "robust_acc_total:  77.7\n",
      "================================================================\n",
      "Epoch: 21\n",
      "robust train accuracy: 77.615 rob train loss: 0.5080514550209045\n",
      "================================================================\n",
      "Training: Average loss: 0.0723, Accuracy: 59229/60000 (99%)\n",
      "robust test loss: 0.5204012648591513\n",
      "natural_acc_total:  98.42\n",
      "robust_acc_total:  78.41\n",
      "================================================================\n",
      "Epoch: 22\n",
      "robust train accuracy: 77.86 rob train loss: 0.5026885271072388\n",
      "================================================================\n",
      "Training: Average loss: 0.0692, Accuracy: 59275/60000 (99%)\n",
      "robust test loss: 0.5107704414219796\n",
      "natural_acc_total:  98.46\n",
      "robust_acc_total:  78.48\n",
      "================================================================\n",
      "Epoch: 23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-10818941f1be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# adversarial training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_adversarial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madversarial_training_entropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_ATENT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madversary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentropySmoothing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epsilon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# evaluation on natural and adversarial examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-99f601168100>\u001b[0m in \u001b[0;36mtrain_adversarial\u001b[0;34m(method, model, device, train_loader, optimizer, epoch, adversary, L, step, eps, norm)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msgd_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# calculate robust loss per batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msgd_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madversary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtotalcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotalloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-d92194263199>\u001b[0m in \u001b[0;36madversarial_training_entropy\u001b[0;34m(model, optimiser, loss_fn, x, y, epoch, adversary, L, step, eps, norm)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss_lg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## initialize model\n",
    "model_ATENT = Net().to(DEVICE)\n",
    "## training params\n",
    "epochs = args['epochs']  \n",
    "lr_init = args['lr']\n",
    "#optimizer = optim.Adam(model_ATENT.parameters(), lr=lr_init)\n",
    "\n",
    "optimizer = optim.SGD(model_ATENT.parameters(), lr=lr_init, momentum=0.9)\n",
    "## train model\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print('Epoch:',epoch)\n",
    "    \n",
    "    # adjust learning rate for SGD\n",
    "    #adjust_learning_rate(optimizer, epoch,lr_init)\n",
    "    \n",
    "    # adversarial training\n",
    "    train_adversarial(adversarial_training_entropy,model_ATENT, DEVICE, train_loader, optimizer, epoch,adversary=entropySmoothing,L=args['num_steps'],step=3*args['step_size'],eps=args['epsilon'],norm=args['norm'])\n",
    "\n",
    "    # evaluation on natural and adversarial examples\n",
    "    print('================================================================')\n",
    "    eval_train(model_ATENT, DEVICE, train_loader)\n",
    "    rob = eval_adv_test_whitebox(model_ATENT, DEVICE, val_loader)            \n",
    "    print('================================================================')\n",
    "\n",
    "    # save checkpoint\n",
    "    if (epoch-1) % 1 == 0:\n",
    "        torch.save(model_ATENT.state_dict(),\n",
    "                   os.path.join(model_dir, 'model-nn-epoch{}-robacc{}.pt'.format(epoch,int(np.round(rob)))))\n",
    "\n",
    "## save model\n",
    "\n",
    "#modelname = '../trainedmodels/'+dataset+'/'+NetName+'_ATENT_'+args['attack']+'_ep'+str(epochs)+'_lr'+str(lr_init)+'.pt'\n",
    "#torch.save(model_ATENT,modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robust test loss: 0.5574548697169823\n",
      "natural_acc_total:  98.71\n",
      "robust_acc_total:  75.05\n"
     ]
    }
   ],
   "source": [
    "rob = eval_adv_test_whitebox(model_ATENT, DEVICE, val_loader)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trades(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate robust loss\n",
    "        loss = trades_loss(model=model,\n",
    "                           x_natural=data,\n",
    "                           y=target,\n",
    "                           optimizer=optimizer,\n",
    "                           step_size=args['step_size'],\n",
    "                           epsilon=args['epsilon'],\n",
    "                           perturb_steps=args['num_steps'],\n",
    "                           beta=args['beta'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "================================================================\n",
      "Training: Average loss: 1.3014, Accuracy: 54627/60000 (91%)\n",
      "robust test loss: 1.884310929081108\n",
      "natural_acc_total:  91.63\n",
      "robust_acc_total:  58.81\n",
      "================================================================\n",
      "Epoch: 2\n",
      "================================================================\n",
      "Training: Average loss: 1.1617, Accuracy: 57130/60000 (95%)\n",
      "robust test loss: 1.8636976540843142\n",
      "natural_acc_total:  95.42\n",
      "robust_acc_total:  65.84\n",
      "================================================================\n",
      "Epoch: 3\n",
      "================================================================\n",
      "Training: Average loss: 1.0932, Accuracy: 57696/60000 (96%)\n",
      "robust test loss: 1.8931925432591499\n",
      "natural_acc_total:  96.28\n",
      "robust_acc_total:  71.15\n",
      "================================================================\n",
      "Epoch: 4\n",
      "================================================================\n",
      "Training: Average loss: 1.0427, Accuracy: 58151/60000 (97%)\n",
      "robust test loss: 1.9963808602924589\n",
      "natural_acc_total:  97.06\n",
      "robust_acc_total:  76.4\n",
      "================================================================\n",
      "Epoch: 5\n",
      "================================================================\n",
      "Training: Average loss: 0.9104, Accuracy: 58493/60000 (97%)\n",
      "robust test loss: 2.0607857734342163\n",
      "natural_acc_total:  97.52\n",
      "robust_acc_total:  76.37\n",
      "================================================================\n",
      "Epoch: 6\n",
      "================================================================\n",
      "Training: Average loss: 0.8221, Accuracy: 58725/60000 (98%)\n",
      "robust test loss: 2.073940759972681\n",
      "natural_acc_total:  98.01\n",
      "robust_acc_total:  71.98\n",
      "================================================================\n",
      "Epoch: 7\n",
      "================================================================\n",
      "Training: Average loss: 0.8216, Accuracy: 58900/60000 (98%)\n",
      "robust test loss: 2.1153879769240755\n",
      "natural_acc_total:  98.21\n",
      "robust_acc_total:  68.25\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "## initialize model\n",
    "model_TRADES = Net().to(DEVICE)\n",
    "## training params\n",
    "lr_init = args['lr']\n",
    "optimizer = optim.SGD(model_TRADES.parameters(), lr=lr_init, momentum=0.9)\n",
    "epochs = args['epochs']\n",
    "## train model\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print('Epoch:',epoch)\n",
    "    # adjust learning rate for SGD\n",
    "    adjust_learning_rate(optimizer, epoch, lr_init = lr_init)\n",
    "\n",
    "    # adversarial training\n",
    "    train_trades(args, model_TRADES, DEVICE, train_loader, optimizer, epoch)\n",
    "\n",
    "    # evaluation on natural examples\n",
    "    if (epoch-1) % 1 == 0:\n",
    "        print('================================================================')\n",
    "        eval_train(model_TRADES, DEVICE, train_loader)\n",
    "        eval_adv_test_whitebox(model_TRADES, DEVICE, val_loader)\n",
    "        print('================================================================')\n",
    "\n",
    "torch.save(model_TRADES.state_dict(),\n",
    "               os.path.join(model_dir, 'TRADES-model-nn-epoch{}-robacc{}.pt'.format(epoch,int(np.round(rob)))))\n",
    "\n",
    "## save model\n",
    "modelname = '../trainedmodels/'+dataset+'/'+NetName+'_TRADES_'+args['attack']+'_ep'+str(epochs)+'_lr'+str(lr_init)+'.pt'\n",
    "#torch.save(model_TRADES,modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_TRADES.state_dict(),\n",
    "               os.path.join(model_dir, 'TRADES-model-nn-epoch{}-robacc{}.pt'.format(epoch,int(np.round(rob)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
